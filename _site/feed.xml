<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://imanyakin.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://imanyakin.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-04-23T23:24:22+01:00</updated><id>https://imanyakin.github.io/feed.xml</id><title type="html">blank</title><subtitle>Attempting to write more
</subtitle><entry><title type="html">Unexpected behaviours of expectations</title><link href="https://imanyakin.github.io/blog/2023/unexpected-behaviours-of-expectations/" rel="alternate" type="text/html" title="Unexpected behaviours of expectations" /><published>2023-04-23T01:00:00+01:00</published><updated>2023-04-23T01:00:00+01:00</updated><id>https://imanyakin.github.io/blog/2023/unexpected-behaviours-of-expectations</id><content type="html" xml:base="https://imanyakin.github.io/blog/2023/unexpected-behaviours-of-expectations/"><![CDATA[<p>The average/mean/expectation is perhaps the most known and commonly used statistical estimator. We tend to use it when we want to estimate the <code class="language-plaintext highlighter-rouge">location</code> of the distribution - its centre.</p>

<p>Most often when faced with a set of values (samples) $x_1,x_2, … , x_n$ (representing instantiations of a random variable $X$) we can estimate the expectation as $\mathbb{E}(X) = \frac{1}{n}\sum_{i=1}^{i=n} x_i$.</p>

<p>When we assume that $X$ is a Gaussian (Normal) distribution, $X \sim Normal(\mu,\sigma^2) = \frac{1}{\sigma \sqrt(2 \pi)} exp(-(x-\mu)^2/\sigma^2)$ the expectation is the mean $\mathbb{E}(X) = \mu$.</p>

<p>If we were to repeat the experiment, drawing values $x_1,…x_n$ multiple times, we would find that our estimate $\mu$ - caused by the (assumed) randomness  of the sampling process. The uncertainty of our estimate, the <code class="language-plaintext highlighter-rouge">standard error</code> is determined by the spread (the standard deviation $\sigma$) of the distribution:</p>

\[\sigma_\mu = \frac{\sigma}{\sqrt{n}}\]

<p>Thus our uncertainty in estimating $\mu$ scales as $O(n^{-1/2})$, meaning that to shrink the uncertainty in the mean by a factor of $\times 10$ we need to sample $n\times100$ values. While merely a curiosity here, we will delve deeper in future posts where this expression will appear in the context of numeric integration using Monte Carlo and Quasi-Monte Carlo methods where we will answer the question “Can we achieve a faster rate converge faster than $O(n^{-1/2})” with a “Yes!”.</p>

<p>Here we will however consider a few things that can go wrong - when does averaging not give us the correct result?</p>

<h1 id="non-symmetric-distributions">Non-symmetric distributions</h1>

<h1 id="outliers-and-breakdown-points">Outliers and breakdown points</h1>

<p>So ok, we can’t hope to get good mean estimators by simple averaging when our distribution is non-symmetric. But surely if our distribution is symmetric and we average long enough - our mean will converge to a value right? Not quite.</p>

<p>Consider what happens when we replace one of the samples $x_i = \infty$. When we compute the mean we get $\mu = \infty$ - the infinity propagates straight through our sum and causes trouble in $\mu$.</p>

<p>What we did here is make our estimator infinitely large by changing a single value - we could have changed more. From this we can introduce the concept of a <code class="language-plaintext highlighter-rouge">breakdown point</code> - the number of infinities that our estimator can handle before it is affects. From above it one can see that the traditional mean has a breakdown point of 0.</p>

<p>What are some other estimators for the location that are <code class="language-plaintext highlighter-rouge">robust</code> to outliers you might ask? Good question - the median is one. This has a breakdown point of 50% (or 0.5) - meaning that the median can tolerance up to 50% of the samples being corrupted (ie. $\pm \infty$) before it is affected.</p>

<h1 id="pathological-distributions---the-gaussian-assumption">Pathological distributions - the Gaussian assumption</h1>

<p>Related to the above discussion of outliers is the assumption that the distribution is Gaussian. A key feature of the Gaussian is that it’s probability density decayed quadratically (the log of the density is a parabola).</p>

<p>While the assumption that a distribution is Gaussian is useful, it may not hold in practice - symmetric distributions for which the mean is undefined exist!</p>

<h2 id="cauchy-distribution">Cauchy distribution</h2>]]></content><author><name></name></author><category term="math" /><category term="stats" /><summary type="html"><![CDATA[Averaging and times when it can go wrong]]></summary></entry></feed>