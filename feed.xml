<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://imanyakin.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://imanyakin.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-04-23T22:28:35+00:00</updated><id>https://imanyakin.github.io/feed.xml</id><title type="html">blank</title><subtitle>Attempting to write more
</subtitle><entry><title type="html">Unexpected behaviours of expectations</title><link href="https://imanyakin.github.io/blog/2023/unexpected-behaviours-of-expectations/" rel="alternate" type="text/html" title="Unexpected behaviours of expectations" /><published>2023-04-23T00:00:00+00:00</published><updated>2023-04-23T00:00:00+00:00</updated><id>https://imanyakin.github.io/blog/2023/unexpected-behaviours-of-expectations</id><content type="html" xml:base="https://imanyakin.github.io/blog/2023/unexpected-behaviours-of-expectations/"><![CDATA[<p>The average/mean/expectation is perhaps the most known and commonly used statistical estimator. We tend to use it when we want to estimate the <code class="language-plaintext highlighter-rouge">location</code> of the distribution - its centre.</p>

<p>Most often when faced with a set of values (samples) <code class="language-plaintext highlighter-rouge">$x_1,x_2, ... , x_n$</code> (representing instantiations of a random variable <code class="language-plaintext highlighter-rouge">$X$</code>) we can estimate the expectation as <code class="language-plaintext highlighter-rouge">$\mathbb{E}(X) = \frac{1}{n}\sum_{i=1}^{i=n} x_i$</code>.</p>

<p>When we assume that <code class="language-plaintext highlighter-rouge">$X$</code> is a Gaussian (Normal) distribution, <code class="language-plaintext highlighter-rouge">$X \sim Normal(\mu,\sigma^2) = \frac{1}{\sigma \sqrt(2 \pi)} exp(-(x-\mu)^2/\sigma^2)$</code> the expectation is the mean $\mathbb{E}(X) = \mu$.</p>

<p>If we were to repeat the experiment, drawing values <code class="language-plaintext highlighter-rouge">$x_1,...x_n$</code> multiple times, we would find that our estimate $\mu$ - caused by the (assumed) randomness  of the sampling process. The uncertainty of our estimate, the <code class="language-plaintext highlighter-rouge">standard error</code> is determined by the spread (the standard deviation $\sigma$) of the distribution:</p>

\[\sigma_\mu = \frac{\sigma}{\sqrt{n}}\]

<p>Thus our uncertainty in estimating <code class="language-plaintext highlighter-rouge">$\mu$</code> scales as <code class="language-plaintext highlighter-rouge">$O(n^{-1/2})$</code>, meaning that to shrink the uncertainty in the mean by a factor of <code class="language-plaintext highlighter-rouge">$\times 10$</code> we need to sample <code class="language-plaintext highlighter-rouge">$n\times100$</code> values. While merely a curiosity here, we will delve deeper in future posts where this expression will appear in the context of numeric integration using Monte Carlo and Quasi-Monte Carlo methods where we will answer the question “Can we achieve a faster rate converge faster than $O(n^{-1/2})” with a “Yes!”.</p>

<p>Here we will however consider a few things that can go wrong - when does averaging not give us the correct result?</p>

<h1 id="non-symmetric-distributions">Non-symmetric distributions</h1>

<h1 id="outliers-and-breakdown-points">Outliers and breakdown points</h1>

<p>So ok, we can’t hope to get good mean estimators by simple averaging when our distribution is non-symmetric. But surely if our distribution is symmetric and we average long enough - our mean will converge to a value right? Not quite.</p>

<p>Consider what happens when we replace one of the samples $x_i = \infty$. When we compute the mean we get $\mu = \infty$ - the infinity propagates straight through our sum and causes trouble in $\mu$.</p>

<p>What we did here is make our estimator infinitely large by changing a single value - we could have changed more. From this we can introduce the concept of a <code class="language-plaintext highlighter-rouge">breakdown point</code> - the number of infinities that our estimator can handle before it is affects. From above it one can see that the traditional mean has a breakdown point of 0.</p>

<p>What are some other estimators for the location that are <code class="language-plaintext highlighter-rouge">robust</code> to outliers you might ask? Good question - the median is one. This has a breakdown point of 50% (or 0.5) - meaning that the median can tolerance up to 50% of the samples being corrupted (ie. $\pm \infty$) before it is affected.</p>

<h1 id="pathological-distributions---the-gaussian-assumption">Pathological distributions - the Gaussian assumption</h1>

<p>Related to the above discussion of outliers is the assumption that the distribution is Gaussian. A key feature of the Gaussian is that it’s probability density decayed quadratically (the log of the density is a parabola).</p>

<p>While the assumption that a distribution is Gaussian is useful, it may not hold in practice - symmetric distributions for which the mean is undefined exist!</p>

<h2 id="cauchy-distribution">Cauchy distribution</h2>

<!-- 
This theme supports rendering beautiful math in inline and display modes using [MathJax 3](https://www.mathjax.org/) engine. You just need to surround your math expression with `$$`, like `$$ E = mc^2 $$`. If you leave it inside a paragraph, it will produce an inline expression, just like $$ E = mc^2 $$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph. Here is an example:

$$
\sum_{k=1}^\infty |\langle x, e_k \rangle|^2 \leq \|x\|^2
$$

You can also use `\begin{equation}...\end{equation}` instead of `$$` for display mode math.
MathJax will automatically number equations:

\begin{equation}
\label{eq:cauchy-schwarz}
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
\end{equation}

and by adding `\label{...}` inside the equation environment, we can now refer to the equation using `\eqref`.

Note that MathJax 3 is [a major re-write of MathJax](https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html) that brought a significant improvement to the loading and rendering speed, which is now [on par with -->
<p>KaTeX](http://www.intmath.com/cg5/katex-mathjax-comparison.php).</p>]]></content><author><name></name></author><category term="math" /><category term="stats" /><summary type="html"><![CDATA[Averaging and times when it can go wrong]]></summary></entry></feed>